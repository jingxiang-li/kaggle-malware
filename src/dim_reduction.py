from heapq import nlargest
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import pickle


def get_labels():
    labels = pd.read_csv(
        '/media/s_ariel/HDD_1T/Kaggle/downloads/trainLabels.csv',
        index_col=0)
    labels.sort_index(inplace=True)
    y = labels['Class'].as_matrix() - 1
    return y


def tfidf_svd(file_path, n_comp=50):
    with open(file_path, 'rb') as f_in:
        feature_list = pickle.load(f_in)

    # ignore malware class 4
    y = get_labels()
    feature_list = [ftr for i, ftr in enumerate(feature_list) if y[i] != 4]
    print(len(feature_list))

    dim_reduction = make_pipeline(
        DictVectorizer(), TfidfTransformer(), TruncatedSVD(algorithm='arpack'))
    dim_reduction.set_params(truncatedsvd__n_components=n_comp)

    dr = dim_reduction.fit(feature_list)
    print(np.sum(dr.named_steps['truncatedsvd'].explained_variance_ratio_))

    X = dr.transform(feature_list)
    return X


def scale_pca(file_path, n_comp=0.95):
    with open(file_path, 'rb') as f_in:
        feature_list = pickle.load(f_in)

    # ignore malware class 4
    y = get_labels()
    feature_list = [ftr for i, ftr in enumerate(feature_list) if y[i] != 4]
    print(len(feature_list))

    dim_reduction = make_pipeline(StandardScaler(), PCA(n_components=n_comp))
    X = dim_reduction.fit_transform(np.array(feature_list))
    return X


def get_file_size():
    with open('../data/size.pickle', 'rb') as f_in:
        X = np.array(pickle.load(f_in))
    X = X.swapaxes(0, 1)
    X = np.int32(X)

    y = get_labels()
    feature_list = [ftr for i, ftr in enumerate(list(X)) if y[i] != 4]
    print(len(feature_list))

    X = np.array(feature_list)
    X = StandardScaler().fit_transform(X)
    print(X.shape)
    return X


feature_set = []

X = tfidf_svd('../data/opcode_1gram.pickle', 100)
feature_set.append(X)

X = tfidf_svd('../data/opcode_2gram.pickle', 200)
feature_set.append(X)

X = tfidf_svd('../data/opcode_3gram.pickle', 300)
feature_set.append(X)

X = tfidf_svd('../data/opcode_4gram.pickle', 500)
feature_set.append(X)

X = tfidf_svd('../data/seg_counter.pickle', 100)
feature_set.append(X)

X = scale_pca('../data/img_gist.pickle', n_comp=0.99)
feature_set.append(X)

X = scale_pca('../data/img_thumb.pickle', n_comp=0.92)
feature_set.append(X)

X = get_file_size()
feature_set.append(X)

X_all = np.concatenate(feature_set, axis=1)

# Feature Selection by Random Forest
y = get_labels()
# ignore class 4
y = [i for i in y if i != 4]
# modify class label
y = [i if i < 4 else i - 1 for i in y]

with open('../data/X_all.pickle', 'wb') as f_out:
    pickle.dump(X_all, f_out)

with open('../data/y_all.pickle', 'wb') as f_out:
    pickle.dump(y, f_out)
